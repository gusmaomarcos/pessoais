{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gusmaomarcos/pessoais/blob/main/twitter_screenshots(deprecated).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "qhMjVxmK-2HL",
        "outputId": "09e66b0f-4cd4-41c5-8ae1-8e3e36749e57"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "'''\n",
        "Twitter export image fill 1.10\n",
        "by Marcin Wichary (aresluna.org)\n",
        "\n",
        "Site: https://github.com/mwichary/twitter-export-image-fill\n",
        "\n",
        "This is free and unencumbered software released into the public domain.\n",
        "Anyone is free to copy, modify, publish, use, compile, sell, or\n",
        "distribute this software, either in source code form or as a compiled\n",
        "binary, for any purpose, commercial or non-commercial, and by any\n",
        "means. For more information, please refer to <http://unlicense.org/>\n",
        "'''\n",
        "\n",
        "# Imports\n",
        "# ---------------------------------\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "from shutil import copyfile\n",
        "# The location of urlretrieve changed modules in Python 3\n",
        "if (sys.version_info > (3, 0)):\n",
        "  from urllib.request import urlretrieve\n",
        "else:\n",
        "  from urllib import urlretrieve\n",
        "\n",
        "\n",
        "def print_intro():\n",
        "  print(\"Twitter export image fill 1.10\")\n",
        "  print(\"by Marcin Wichary (aresluna.org) and others\")\n",
        "  print(\"use --help to see options\")\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "def parse_arguments():\n",
        "  parser = argparse.ArgumentParser(description = 'Downloads all the images to your Twitter archive .')\n",
        "  parser.add_argument('--include-videos', dest='PATH_TO_YOUTUBE_DL',\n",
        "      help = 'use youtube_dl to download videos (and animated GIFs) in addition to images')\n",
        "  parser.add_argument('--continue-after-failure', action='store_true',\n",
        "      help = 'continue the process when one of the downloads fail (creates incomplete archive)')\n",
        "  parser.add_argument('--backfill-from', dest='EARLIER_ARCHIVE_PATH',\n",
        "      help = 'copy images downloaded into an earlier archive instead of downloading them again (useful for incremental backups)')\n",
        "  parser.add_argument('--skip-retweets', action='store_true',\n",
        "      help = 'do not download images or videos from retweets')\n",
        "  parser.add_argument('--skip-images', action='store_true',\n",
        "      help = 'do not download images in general')\n",
        "  parser.add_argument('--skip-videos', action='store_true',\n",
        "      help = 'do not download videos (and animated GIFs) in general')\n",
        "  parser.add_argument('--skip-avatars', action='store_true',\n",
        "      help = 'do not download avatar images')\n",
        "  parser.add_argument('--verbose', action='store_true',\n",
        "      help = 'show additional debugging info')\n",
        "  parser.add_argument('--force-redownload', action='store_true',\n",
        "      help = 'force to re-download images and videos that were already downloaded')\n",
        "  return parser.parse_args()\n",
        "\n",
        "\n",
        "def find_youtube_dl():\n",
        "  if not args.skip_videos:\n",
        "    if args.PATH_TO_YOUTUBE_DL:\n",
        "      if not os.path.isfile(args.PATH_TO_YOUTUBE_DL):\n",
        "        print(\"Could not find youtube-dl executable.\")\n",
        "        print(\"Make sure you're pointing at the right file.\")\n",
        "        print(\"A typical path would be: /usr/local/bin/youtube-dl\")\n",
        "        sys.exit(-1)\n",
        "\n",
        "      return True, args.PATH_TO_YOUTUBE_DL\n",
        "    else:\n",
        "      if os.path.isfile('/usr/local/bin/youtube-dl'):\n",
        "        if args.verbose:\n",
        "          print(\"(Found youtube-dl automatically.)\")\n",
        "          print(\"\")\n",
        "        return True, '/usr/local/bin/youtube-dl'\n",
        "  return False, ''\n",
        "\n",
        "\n",
        "def test_earlier_archive_path():\n",
        "  if args.EARLIER_ARCHIVE_PATH:\n",
        "    earlier_archive_path = args.EARLIER_ARCHIVE_PATH\n",
        "    # Normalizes the slash at the end so it supports both including and not including it\n",
        "    earlier_archive_path = earlier_archive_path.rstrip('/') + '/'\n",
        "    if not os.path.isfile(earlier_archive_path + '/data/js/tweet_index.js'):\n",
        "      print(\"Could not find the earlier archive!\")\n",
        "      print(\"Make sure you're pointing at the directory that contains the index.html file.\")\n",
        "      sys.exit(-1)\n",
        "    return earlier_archive_path\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "\n",
        "def load_tweet_index():\n",
        "  index_filename = \"data/js/tweet_index.js\"\n",
        "  try:\n",
        "    with open(index_filename) as index_file:\n",
        "      index_str = index_file.read()\n",
        "      index_str = re.sub(r'var tweet_index =', '', index_str)\n",
        "      return json.loads(index_str)\n",
        "\n",
        "  except:\n",
        "    print(\"Could not open the data file!\")\n",
        "    print(\"Please run this script from your tweet archive directory\")\n",
        "    print(\"(the one with index.html file).\")\n",
        "    print(\"\")\n",
        "    sys.exit(-1)\n",
        "\n",
        "\n",
        "def make_directory_if_needed(directory_path):\n",
        "  if not os.path.isdir(directory_path):\n",
        "    os.mkdir(directory_path)\n",
        "\n",
        "\n",
        "def is_retweet(tweet):\n",
        "  return 'retweeted_status' in tweet.keys()\n",
        "\n",
        "\n",
        "def output_line(line):\n",
        "  sys.stdout.write(\"\\r%s\\033[K\" % line) # Clears the end of the line\n",
        "  sys.stdout.flush()\n",
        "\n",
        "\n",
        "# Re-save the JSON data back to the original file.\n",
        "def resave_data(data, data_filename, first_data_line, year_str, month_str):\n",
        "  # Writing to a separate file so that we can only copy over the\n",
        "  # main file when done\n",
        "  data_filename_temp = 'data/js/tweets/%s_%s.js.tmp' % (year_str, month_str)\n",
        "  with open(data_filename_temp, 'w') as f:\n",
        "    f.write(first_data_line)\n",
        "    json.dump(data, f, indent=2)\n",
        "  os.remove(data_filename)\n",
        "  os.rename(data_filename_temp, data_filename)\n",
        "\n",
        "\n",
        "# Download a given image directly from the URL\n",
        "def download_image(url, local_filename):\n",
        "  if not download_images:\n",
        "    return True\n",
        "\n",
        "  try:\n",
        "    urlretrieve(url, local_filename)\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "\n",
        "# Download a given video via youtube-dl\n",
        "def download_video(url, local_filename):\n",
        "  if not download_videos:\n",
        "    return True\n",
        "\n",
        "  try:\n",
        "    local_filename_escaped = local_filename.replace(' ', '\\ ')\n",
        "    command = '%s -q --no-warnings %s --exec \\'mv {} %s\\' &>/dev/null' % \\\n",
        "        (youtube_dl_path, url, local_filename_escaped)\n",
        "    if os.system(command) > 0:\n",
        "      return False\n",
        "    if os.path.isfile(local_filename):\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "\n",
        "# Downloads an avatar image for a tweet.\n",
        "# @return Whether data was rewritten\n",
        "def download_or_copy_avatar(user, total_image_count, total_video_count, total_media_precount, year_str, month_str):\n",
        "  # _orig existing means we already processed this user\n",
        "  if 'profile_image_url_https_orig' in user:\n",
        "    return False\n",
        "\n",
        "  avatar_url = user['profile_image_url_https']\n",
        "  extension = os.path.splitext(avatar_url)[1]\n",
        "  local_filename = \"img/avatars/%s%s\" % (user['screen_name'], extension)\n",
        "\n",
        "  if not os.path.isfile(local_filename):\n",
        "    can_be_copied = args.EARLIER_ARCHIVE_PATH and os.path.isfile(earlier_archive_path + local_filename)\n",
        "\n",
        "    output_line(\"[%0.1f%%] %s/%s: %s avatar...\" %\n",
        "      ((total_image_count + total_video_count) / total_media_precount * 100, \\\n",
        "      year_str, month_str, \\\n",
        "      \"Copying\" if can_be_copied else \"Downloading\"))\n",
        "\n",
        "    # If using an earlier archive as a starting point, try to see if the\n",
        "    # avatar image is there and can be copied\n",
        "    if can_be_copied:\n",
        "      copyfile(earlier_archive_path + local_filename, local_filename)\n",
        "    # Otherwise download it\n",
        "    else:\n",
        "      try:\n",
        "        urlretrieve(avatar_url, local_filename)\n",
        "      except:\n",
        "        # Okay to quietly fail, this is just an avatar\n",
        "        # (And, apparently, some avatars return 404.)\n",
        "        return False\n",
        "\n",
        "  user['profile_image_url_https_orig'] = user['profile_image_url_https']\n",
        "  user['profile_image_url_https'] = local_filename\n",
        "  return True\n",
        "\n",
        "\n",
        "def download_file(url, local_filename, is_video, tweet):\n",
        "  downloaded = False\n",
        "  download_tries = 3\n",
        "  while not downloaded:\n",
        "    if (is_video and download_video(url, local_filename)) or \\\n",
        "       (not is_video and download_image(url, local_filename)):\n",
        "      return True\n",
        "    else:\n",
        "      download_tries = download_tries - 1\n",
        "      if download_tries == 0:\n",
        "        if not args.continue_after_failure:\n",
        "          print(\"\")\n",
        "          print(\"\")\n",
        "          print(\"Failed to download %s after 3 tries.\" % url)\n",
        "          print(\"Please try again later?\")\n",
        "          print(\"(Alternatively, use --continue-after-failure option to skip past failed files.)\")\n",
        "          # Output debugging info if needed\n",
        "          if args.verbose:\n",
        "            print(\"Debug info: Tweet ID = %s \" % tweet['id'])\n",
        "          sys.exit(-2)\n",
        "        else:\n",
        "          failed_files.append((tweet['id'], url))\n",
        "          return False\n",
        "      time.sleep(3) # Wait 3 seconds before retrying\n",
        "      sys.stdout.write(\".\")\n",
        "      sys.stdout.flush()\n",
        "\n",
        "\n",
        "def determine_image_or_video(medium, year_str, month_str, date, tweet, tweet_media_count):\n",
        "  # Video\n",
        "  if '/video/' in medium['expanded_url']:\n",
        "    is_video = True\n",
        "    separator = '-video'\n",
        "    url = medium['expanded_url']\n",
        "    extension = '.mp4'\n",
        "  # Animated GIF transcoded into a video\n",
        "  elif 'tweet_video_thumb' in medium['media_url']:\n",
        "    is_video = True\n",
        "    separator = '-gif-video'\n",
        "    id = re.match(r'(.*)tweet_video_thumb/(.*)\\.', medium['media_url']).group(2)\n",
        "    url = \"https://video.twimg.com/tweet_video/%s.mp4\" % id\n",
        "    extension = os.path.splitext(url)[1]\n",
        "  # Regular non-animated image\n",
        "  else:\n",
        "    is_video = False\n",
        "    separator = ''\n",
        "    url = medium['media_url_https']\n",
        "    extension = os.path.splitext(url)[1]\n",
        "    # Download the original/best image size, rather than the default one\n",
        "    url = url + ':orig'\n",
        "\n",
        "  local_filename = 'data/js/tweets/%s_%s_media/%s-%s%s-%s%s%s' % \\\n",
        "      (year_str, month_str, date, tweet['id'], separator,\n",
        "      'rt-' if is_retweet(tweet) else '', tweet_media_count, extension)\n",
        "\n",
        "  return is_video, url, local_filename\n",
        "\n",
        "\n",
        "def process_tweets(tweets_by_month, trial_run, total_media_precount=None):\n",
        "  total_image_count = 0\n",
        "  total_video_count = 0\n",
        "\n",
        "  # Loop 1: Go through all the months\n",
        "  # ---------------------------------\n",
        "\n",
        "  for date in tweets_by_month:\n",
        "    try:\n",
        "      year_str = '%04d' % date['year']\n",
        "      month_str = '%02d' % date['month']\n",
        "      data_filename = 'data/js/tweets/%s_%s.js' % (year_str, month_str)\n",
        "\n",
        "      if not trial_run:\n",
        "        # Make a copy of the original JS file, just in case (only if it doesn't exist before)\n",
        "        backup_filename = 'data/js/tweets/%s_%s_original.js' % (year_str, month_str)\n",
        "        if not os.path.isfile(backup_filename):\n",
        "          copyfile(data_filename, backup_filename)\n",
        "\n",
        "\n",
        "      # Loop 2: Go through all the tweets in a month\n",
        "      # --------------------------------------------\n",
        "\n",
        "      with open(data_filename) as data_file:\n",
        "        data_str = data_file.read()\n",
        "        # Remove the assignment to a variable that breaks JSON parsing,\n",
        "        # but save for later since we have to recreate the file\n",
        "        first_data_line = re.match(r'Grailbird.data.tweets_(.*) =', data_str).group(0)\n",
        "        data_str = re.sub(first_data_line, '', data_str)\n",
        "        data = json.loads(data_str)\n",
        "\n",
        "      tweet_length = len(data)\n",
        "      month_media_count = 0\n",
        "      directory_name = 'data/js/tweets/%s_%s_media' % (year_str, month_str)\n",
        "\n",
        "      for tweet in data:\n",
        "        # Output the string just for the sake of status continuity\n",
        "        if not trial_run:\n",
        "          output_line(\"[%0.1f%%] %s/%s: Analyzing avatars...\" %\n",
        "              ((total_image_count + total_video_count) / total_media_precount * 100, year_str, month_str))\n",
        "\n",
        "        # Before downloading any images, download an avatar for tweet's author\n",
        "        # (same for retweet if asked to)\n",
        "        if not trial_run and not args.skip_avatars:\n",
        "          data_changed = download_or_copy_avatar(tweet['user'], \\\n",
        "              total_image_count, total_video_count, total_media_precount, year_str, month_str)\n",
        "\n",
        "          data_changed_retweet = False\n",
        "          if not args.skip_retweets and is_retweet(tweet):\n",
        "            data_changed_retweet = download_or_copy_avatar(tweet['retweeted_status']['user'], \\\n",
        "                total_image_count, total_video_count, total_media_precount, year_str, month_str)\n",
        "\n",
        "          # Re-save the JSON file if we grabbed any avatars\n",
        "          if data_changed or data_changed_retweet:\n",
        "            resave_data(data, data_filename, first_data_line, year_str, month_str)\n",
        "\n",
        "        # Don't continue with saving images if a retweet (unless forced to)\n",
        "        if (args.skip_retweets) and is_retweet(tweet):\n",
        "          continue\n",
        "\n",
        "        media = tweet['retweeted_status']['entities']['media'] if is_retweet(tweet) \\\n",
        "          else tweet['entities']['media']\n",
        "\n",
        "        if media:\n",
        "          tweet_media_count = 1\n",
        "\n",
        "          # Rewrite tweet date to be used in the filename prefix\n",
        "          # (only first 19 characters + replace colons with dots)\n",
        "          date = re.sub(r':', '.', tweet['created_at'][:19])\n",
        "\n",
        "\n",
        "          # Loop 3: Go through all the media in a tweet\n",
        "          # -------------------------------------------\n",
        "\n",
        "          for medium in media:\n",
        "            # media_url_orig being present means we already processed/downloaded\n",
        "            # this image or video\n",
        "            if 'media_url_orig' in medium.keys() and not args.force_redownload:\n",
        "              continue\n",
        "\n",
        "            # If forcing download, the above has to be undone.\n",
        "            if args.force_redownload and 'media_url_orig' in medium.keys():\n",
        "              medium['media_url'] = medium['media_url_orig']\n",
        "\n",
        "            is_video, url, local_filename = \\\n",
        "                determine_image_or_video(medium, year_str, month_str, date, tweet, tweet_media_count)\n",
        "\n",
        "            if not trial_run:\n",
        "              # If using an earlier archive as a starting point, try to find the desired\n",
        "              # image file there first, and copy it if present\n",
        "              can_be_copied = args.EARLIER_ARCHIVE_PATH and os.path.isfile(earlier_archive_path + local_filename)\n",
        "\n",
        "              output_line(\"[%0.1f%%] %s/%s: %s %s %s...\" %\n",
        "                  ((total_image_count + total_video_count) / total_media_precount * 100, \\\n",
        "                  year_str, month_str, \\\n",
        "                  \"Copying\" if can_be_copied else \"Downloading\", \\\n",
        "                  \"video\" if is_video else \"image\", url.split('/')[-1]))\n",
        "\n",
        "              # Only make the directory when we're ready to write the first file;\n",
        "              # this will avoid empty directories\n",
        "              make_directory_if_needed(directory_name)\n",
        "\n",
        "              success = False\n",
        "              if can_be_copied:\n",
        "                copyfile(earlier_archive_path + local_filename, local_filename)\n",
        "                success = True\n",
        "              else:\n",
        "                success = download_file(url, local_filename, is_video, tweet)\n",
        "\n",
        "              # Change the URL so that the archive's index.html will now point to the\n",
        "              # just-downloaded local file...\n",
        "              if success and ((not is_video and download_images) or (is_video and download_videos)):\n",
        "                medium['media_url_orig'] = medium['media_url']\n",
        "                medium['media_url'] = local_filename\n",
        "\n",
        "                # Re-save the original JSON file every time, so that the script can continue\n",
        "                # from this point\n",
        "                resave_data(data, data_filename, first_data_line, year_str, month_str)\n",
        "\n",
        "            tweet_media_count += 1\n",
        "            month_media_count += 1\n",
        "\n",
        "            if is_video:\n",
        "              total_video_count += 1\n",
        "            else:\n",
        "              total_image_count += 1\n",
        "\n",
        "          # End loop 3 (images in a tweet)\n",
        "\n",
        "      # End loop 2 (tweets in a month)\n",
        "      if not trial_run and month_media_count:\n",
        "        output_line(\"%i images/videos downloaded from %s/%s.\" % (month_media_count, year_str, month_str))\n",
        "        print(\"\")\n",
        "\n",
        "    # Nicer support for Ctrl-C\n",
        "    except KeyboardInterrupt:\n",
        "      print(\"\")\n",
        "      print(\"Interrupted! Come back any time.\")\n",
        "      sys.exit(-3)\n",
        "\n",
        "  # End loop 1 (all the months)\n",
        "  return total_image_count, total_video_count\n",
        "\n",
        "\n",
        "# Main entry point\n",
        "# ---------------------------------\n",
        "\n",
        "print_intro()\n",
        "\n",
        "# Process arguments, find components\n",
        "\n",
        "args = parse_arguments()\n",
        "download_images = not args.skip_images\n",
        "download_videos, youtube_dl_path = find_youtube_dl()\n",
        "\n",
        "# Check whether the earlier archive actually exists\n",
        "# (This is important because failure would mean quietly downloading all the files again)\n",
        "\n",
        "earlier_archive_path = test_earlier_archive_path()\n",
        "\n",
        "# Prepare environment, etc.\n",
        "\n",
        "if not args.skip_avatars:\n",
        "  make_directory_if_needed(\"img/avatars\")\n",
        "if args.continue_after_failure:\n",
        "  failed_files = []\n",
        "\n",
        "# Process the index file\n",
        "\n",
        "tweets_by_month = load_tweet_index()\n",
        "month_count = len(tweets_by_month)\n",
        "\n",
        "# Scan the file to know how much work needs to be done\n",
        "\n",
        "print(\"Scanning...\")\n",
        "total_image_precount, total_video_precount = \\\n",
        "    process_tweets(tweets_by_month, True)\n",
        "total_media_precount = total_image_precount + total_video_precount\n",
        "\n",
        "print(\"\")\n",
        "if not args.skip_images and not args.skip_videos:\n",
        "  print(\"To process: %i months' worth of tweets with %i images and %i videos.\" % \\\n",
        "      (month_count, total_image_precount, total_video_precount))\n",
        "elif not args.skip_images:\n",
        "  print(\"To process: %i months' worth of tweets with %i images.\" % \\\n",
        "      (month_count, total_image_precount))\n",
        "elif not args.skip_videos:\n",
        "  print(\"To process: %i months' worth of tweets with %i videos.\" % \\\n",
        "      (month_count, total_video_precount))\n",
        "print(\"(You can cancel any time. Next time you run, the script should resume at the last point.)\")\n",
        "print(\"\")\n",
        "\n",
        "# Actually download everything\n",
        "\n",
        "total_image_count, total_video_count = \\\n",
        "    process_tweets(tweets_by_month, False, total_media_precount)\n",
        "\n",
        "# Communicate success\n",
        "\n",
        "print(\"\")\n",
        "print(\"Done!\")\n",
        "if download_images:\n",
        "  print(\"%i images downloaded in total.\" % total_image_count)\n",
        "if download_videos:\n",
        "  print(\"%i videos downloaded in total.\" % total_video_count)\n",
        "print(\"\")\n",
        "\n",
        "if args.continue_after_failure and len(failed_files):\n",
        "  print(\"%i files have **NOT** been downloaded. Here are the URLs:\" % len(failed_files))\n",
        "  for line in failed_files:\n",
        "    print(\"- https://twitter.com/user/status/%s: %s\" % line)\n",
        "  print(\"\")\n",
        "\n",
        "if total_video_count and not download_videos:\n",
        "  print(\"%i videos have **NOT** been downloaded.\" % total_video_count)\n",
        "  print(\"If you want, use the include-videos option to download videos.\")\n",
        "  print(\"For more info, use --help, or look at https://github.com/mwichary/twitter-export-image-fill\")\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Twitter export image fill 1.10\n",
            "by Marcin Wichary (aresluna.org) and others\n",
            "use --help to see options\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--include-videos PATH_TO_YOUTUBE_DL]\n",
            "                             [--continue-after-failure]\n",
            "                             [--backfill-from EARLIER_ARCHIVE_PATH]\n",
            "                             [--skip-retweets] [--skip-images] [--skip-videos]\n",
            "                             [--skip-avatars] [--verbose] [--force-redownload]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-7597399a-caac-42b5-8821-2c957251e7fe.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB4xYA2M_zy9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}